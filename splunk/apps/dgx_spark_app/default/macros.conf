##############################################################################
# DGX Spark Command Center - Splunk Search Macros
##############################################################################

#=============================================================================
# Index Shortcuts
#=============================================================================

[dgx_metrics]
definition = index=dgx_spark_metrics
iseval = 0

[dgx_logs]
definition = index=dgx_spark_logs
iseval = 0

[dgx_containers]
definition = index=dgx_spark_containers
iseval = 0

[dgx_apps]
definition = index=dgx_spark_apps
iseval = 0

[dgx_inference]
definition = index=dgx_spark_inference
iseval = 0

[dgx_all]
definition = index=dgx_spark_metrics OR index=dgx_spark_logs OR index=dgx_spark_containers OR index=dgx_spark_apps OR index=dgx_spark_inference
iseval = 0

#=============================================================================
# GPU Metrics Macros
#=============================================================================

[gpu_utilization]
definition = `dgx_metrics` sourcetype=dcgm:metrics | stats avg(gpu_utilization) as avg_util, max(gpu_utilization) as max_util by host, gpu
iseval = 0

[gpu_temperature]
definition = `dgx_metrics` sourcetype=dcgm:metrics | stats avg(temperature) as avg_temp, max(temperature) as max_temp by host, gpu
iseval = 0

[gpu_memory]
definition = `dgx_metrics` sourcetype=dcgm:metrics | eval memory_total=fb_memory_used+fb_memory_free | eval memory_util=(fb_memory_used/memory_total)*100 | stats avg(memory_util) as avg_mem_util, max(fb_memory_used) as max_mem_used by host, gpu
iseval = 0

[gpu_power]
definition = `dgx_metrics` sourcetype=dcgm:metrics | stats avg(power_usage) as avg_power, max(power_usage) as max_power, sum(power_usage) as total_power by host
iseval = 0

#=============================================================================
# Cluster Health Macros
#=============================================================================

[cluster_health]
definition = `dgx_metrics` sourcetype=dcgm:metrics | stats dc(host) as nodes_reporting, avg(gpu_utilization) as cluster_gpu_util, avg(temperature) as cluster_avg_temp, sum(power_usage) as cluster_power_watts | eval cluster_status=if(nodes_reporting>=2, "healthy", "degraded")
iseval = 0

[node_status(1)]
definition = `dgx_metrics` sourcetype=dcgm:metrics host=$node$ | stats latest(gpu_utilization) as gpu_util, latest(temperature) as gpu_temp, latest(power_usage) as power_draw, latest(fb_memory_used) as mem_used by gpu
iseval = 0
args = node

#=============================================================================
# Spark Job Macros
#=============================================================================

[spark_jobs]
definition = `dgx_apps` sourcetype=spark:driver | rex "(?<job_id>job_\d+)" | stats count by job_id, host
iseval = 0

[spark_errors]
definition = `dgx_apps` sourcetype=spark:* log_level=ERROR
iseval = 0

#=============================================================================
# Inference Macros
#=============================================================================

[inference_latency]
definition = `dgx_inference` sourcetype=vllm:server | stats avg(latency_ms) as avg_latency, p95(latency_ms) as p95_latency, p99(latency_ms) as p99_latency by model
iseval = 0

[inference_throughput]
definition = `dgx_inference` sourcetype=vllm:server | stats count as requests, sum(completion_tokens) as total_tokens by model | eval tokens_per_request=total_tokens/requests
iseval = 0

#=============================================================================
# Alert Threshold Macros
#=============================================================================

[gpu_temp_warning_threshold]
definition = 75
iseval = 0

[gpu_temp_critical_threshold]
definition = 85
iseval = 0

[gpu_memory_warning_threshold]
definition = 85
iseval = 0

[gpu_memory_critical_threshold]
definition = 95
iseval = 0

[gpu_power_warning_threshold]
definition = 250
iseval = 0
