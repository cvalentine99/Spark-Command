##############################################################################
# DGX Spark Command Center - Splunk Saved Searches and Alerts
##############################################################################

#=============================================================================
# CRITICAL ALERTS - Immediate Action Required
#=============================================================================

[DGX Spark - Critical: GPU Temperature Exceeded]
description = Alert when GPU temperature exceeds 85°C threshold
search = index=dgx_spark_metrics sourcetype=dcgm:metrics temperature>85 | stats max(temperature) as max_temp, values(gpu) as gpus by host | where max_temp>85
cron_schedule = */2 * * * *
enableSched = 1
dispatch.earliest_time = -5m
dispatch.latest_time = now
alert_type = number of events
alert_comparator = greater than
alert_threshold = 0
alert.severity = 5
alert.suppress = 1
alert.suppress.period = 15m
alert.suppress.fields = host
action.email = 1
action.email.to = dgx-alerts@example.com
action.email.subject = [CRITICAL] DGX Spark GPU Temperature Alert - $result.host$
action.email.message.alert = GPU temperature on $result.host$ has exceeded 85°C. Max temperature: $result.max_temp$°C on GPUs: $result.gpus$. Immediate investigation required.
action.webhook = 1
action.webhook.param.url = https://events.pagerduty.com/v2/enqueue

[DGX Spark - Critical: Node Offline]
description = Alert when a DGX Spark node stops reporting metrics
search = | tstats count WHERE index=dgx_spark_metrics sourcetype=dcgm:metrics by host | appendpipe [| stats count | eval host="dgx-spark-01", count=0 | where count=0] | appendpipe [| stats count | eval host="dgx-spark-02", count=0 | where count=0] | where count=0
cron_schedule = */5 * * * *
enableSched = 1
dispatch.earliest_time = -10m
dispatch.latest_time = now
alert_type = number of events
alert_comparator = greater than
alert_threshold = 0
alert.severity = 5
alert.suppress = 1
alert.suppress.period = 30m
alert.suppress.fields = host
action.email = 1
action.email.to = dgx-alerts@example.com
action.email.subject = [CRITICAL] DGX Spark Node Offline - $result.host$
action.webhook = 1
action.webhook.param.url = https://events.pagerduty.com/v2/enqueue

[DGX Spark - Critical: GPU XID Error Detected]
description = Alert on NVIDIA XID errors indicating GPU hardware issues
search = index=dgx_spark_logs sourcetype=nvidia:driver "NVRM" "Xid" | rex "Xid.*:\s*(?<xid_code>\d+)" | stats count, values(xid_code) as xid_codes by host
cron_schedule = */5 * * * *
enableSched = 1
dispatch.earliest_time = -10m
dispatch.latest_time = now
alert_type = number of events
alert_comparator = greater than
alert_threshold = 0
alert.severity = 5
alert.suppress = 1
alert.suppress.period = 1h
alert.suppress.fields = host
action.email = 1
action.email.to = dgx-alerts@example.com
action.email.subject = [CRITICAL] DGX Spark XID Error - $result.host$
action.email.message.alert = XID errors detected on $result.host$. XID codes: $result.xid_codes$. Count: $result.count$. This may indicate GPU hardware issues.
action.webhook = 1
action.webhook.param.url = https://events.pagerduty.com/v2/enqueue

[DGX Spark - Critical: GPU Memory Exhausted]
description = Alert when GPU memory utilization exceeds 95%
search = index=dgx_spark_metrics sourcetype=dcgm:metrics | eval mem_util=(fb_memory_used/(fb_memory_used+fb_memory_free))*100 | where mem_util>95 | stats max(mem_util) as max_mem_util, values(gpu) as gpus by host
cron_schedule = */2 * * * *
enableSched = 1
dispatch.earliest_time = -5m
dispatch.latest_time = now
alert_type = number of events
alert_comparator = greater than
alert_threshold = 0
alert.severity = 5
alert.suppress = 1
alert.suppress.period = 15m
alert.suppress.fields = host
action.email = 1
action.email.to = dgx-alerts@example.com
action.email.subject = [CRITICAL] DGX Spark GPU Memory Exhausted - $result.host$
action.webhook = 1
action.webhook.param.url = https://events.pagerduty.com/v2/enqueue

#=============================================================================
# HIGH PRIORITY ALERTS - Requires Attention
#=============================================================================

[DGX Spark - High: GPU Temperature Warning]
description = Warning when GPU temperature exceeds 75°C
search = index=dgx_spark_metrics sourcetype=dcgm:metrics temperature>75 temperature<=85 | stats max(temperature) as max_temp, values(gpu) as gpus by host | where max_temp>75
cron_schedule = */5 * * * *
enableSched = 1
dispatch.earliest_time = -10m
dispatch.latest_time = now
alert_type = number of events
alert_comparator = greater than
alert_threshold = 0
alert.severity = 4
alert.suppress = 1
alert.suppress.period = 30m
alert.suppress.fields = host
action.email = 1
action.email.to = dgx-alerts@example.com
action.email.subject = [WARNING] DGX Spark GPU Temperature Elevated - $result.host$
action.slack = 1
action.slack.param.channel = #dgx-spark-alerts
action.slack.param.message = :warning: GPU temperature warning on $result.host$. Max temp: $result.max_temp$°C

[DGX Spark - High: Power Draw Exceeded]
description = Alert when total power draw exceeds 265W per node
search = index=dgx_spark_metrics sourcetype=dcgm:metrics | stats sum(power_usage) as total_power by host | where total_power>265
cron_schedule = */5 * * * *
enableSched = 1
dispatch.earliest_time = -10m
dispatch.latest_time = now
alert_type = number of events
alert_comparator = greater than
alert_threshold = 0
alert.severity = 4
alert.suppress = 1
alert.suppress.period = 30m
alert.suppress.fields = host
action.email = 1
action.email.to = dgx-alerts@example.com
action.email.subject = [WARNING] DGX Spark High Power Draw - $result.host$
action.slack = 1
action.slack.param.channel = #dgx-spark-alerts

[DGX Spark - High: Spark Job Failed]
description = Alert when a Spark job fails
search = index=dgx_spark_apps sourcetype=spark:driver "Job failed" | rex "job_(?<job_id>\d+)" | stats count, values(job_id) as failed_jobs by host
cron_schedule = */5 * * * *
enableSched = 1
dispatch.earliest_time = -10m
dispatch.latest_time = now
alert_type = number of events
alert_comparator = greater than
alert_threshold = 0
alert.severity = 4
alert.suppress = 1
alert.suppress.period = 15m
alert.suppress.fields = host
action.email = 1
action.email.to = dgx-alerts@example.com
action.email.subject = [WARNING] DGX Spark Job Failed - $result.host$
action.slack = 1
action.slack.param.channel = #dgx-spark-alerts
action.slack.param.message = :x: Spark job failed on $result.host$. Failed jobs: $result.failed_jobs$

[DGX Spark - High: Inference Latency Spike]
description = Alert when inference P95 latency exceeds 2 seconds
search = index=dgx_spark_inference sourcetype=vllm:server | stats p95(latency_ms) as p95_latency by model | where p95_latency>2000
cron_schedule = */5 * * * *
enableSched = 1
dispatch.earliest_time = -15m
dispatch.latest_time = now
alert_type = number of events
alert_comparator = greater than
alert_threshold = 0
alert.severity = 4
alert.suppress = 1
alert.suppress.period = 30m
alert.suppress.fields = model
action.email = 1
action.email.to = dgx-alerts@example.com
action.email.subject = [WARNING] DGX Spark Inference Latency Spike - $result.model$
action.slack = 1
action.slack.param.channel = #dgx-spark-alerts

#=============================================================================
# MEDIUM PRIORITY ALERTS - Monitoring
#=============================================================================

[DGX Spark - Medium: GPU Memory Warning]
description = Warning when GPU memory utilization exceeds 85%
search = index=dgx_spark_metrics sourcetype=dcgm:metrics | eval mem_util=(fb_memory_used/(fb_memory_used+fb_memory_free))*100 | where mem_util>85 AND mem_util<=95 | stats max(mem_util) as max_mem_util by host
cron_schedule = */10 * * * *
enableSched = 1
dispatch.earliest_time = -15m
dispatch.latest_time = now
alert_type = number of events
alert_comparator = greater than
alert_threshold = 0
alert.severity = 3
alert.suppress = 1
alert.suppress.period = 1h
alert.suppress.fields = host
action.slack = 1
action.slack.param.channel = #dgx-spark-alerts
action.slack.param.message = :warning: GPU memory utilization at $result.max_mem_util$% on $result.host$

[DGX Spark - Medium: High Error Rate in Logs]
description = Alert when error rate in Spark logs exceeds 5%
search = index=dgx_spark_apps sourcetype=spark:* | stats count(eval(log_level="ERROR")) as errors, count as total | eval error_rate=(errors/total)*100 | where error_rate>5
cron_schedule = */15 * * * *
enableSched = 1
dispatch.earliest_time = -30m
dispatch.latest_time = now
alert_type = number of events
alert_comparator = greater than
alert_threshold = 0
alert.severity = 3
alert.suppress = 1
alert.suppress.period = 1h
action.slack = 1
action.slack.param.channel = #dgx-spark-alerts

[DGX Spark - Medium: Network Interface Errors]
description = Alert on network interface errors
search = index=dgx_spark_metrics sourcetype=network:stats (rx_errors>0 OR tx_errors>0) | stats sum(rx_errors) as total_rx_errors, sum(tx_errors) as total_tx_errors by host, interface
cron_schedule = */15 * * * *
enableSched = 1
dispatch.earliest_time = -30m
dispatch.latest_time = now
alert_type = number of events
alert_comparator = greater than
alert_threshold = 0
alert.severity = 3
alert.suppress = 1
alert.suppress.period = 2h
alert.suppress.fields = host,interface
action.slack = 1
action.slack.param.channel = #dgx-spark-alerts

#=============================================================================
# SCHEDULED REPORTS
#=============================================================================

[DGX Spark - Daily Cluster Health Report]
description = Daily summary of cluster health metrics
search = index=dgx_spark_metrics sourcetype=dcgm:metrics | stats avg(gpu_utilization) as avg_gpu_util, max(temperature) as max_temp, avg(power_usage) as avg_power, dc(host) as nodes_reporting by host | stats avg(avg_gpu_util) as cluster_avg_util, max(max_temp) as cluster_max_temp, sum(avg_power) as cluster_total_power, sum(nodes_reporting) as total_nodes
cron_schedule = 0 8 * * *
enableSched = 1
dispatch.earliest_time = -24h
dispatch.latest_time = now
action.email = 1
action.email.to = dgx-reports@example.com
action.email.subject = DGX Spark Daily Health Report - $now$
action.email.format = html
action.email.inline = 1

[DGX Spark - Weekly Inference Performance Report]
description = Weekly summary of inference performance
search = index=dgx_spark_inference sourcetype=vllm:server | stats count as total_requests, avg(latency_ms) as avg_latency, p95(latency_ms) as p95_latency, avg(tokens_per_sec) as avg_tps, sum(completion_tokens) as total_tokens by model
cron_schedule = 0 9 * * 1
enableSched = 1
dispatch.earliest_time = -7d
dispatch.latest_time = now
action.email = 1
action.email.to = dgx-reports@example.com
action.email.subject = DGX Spark Weekly Inference Report
action.email.format = html
action.email.inline = 1

[DGX Spark - Monthly Utilization Summary]
description = Monthly summary of cluster utilization
search = index=dgx_spark_metrics sourcetype=dcgm:metrics | timechart span=1d avg(gpu_utilization) as daily_avg_util, sum(power_usage) as daily_power by host | stats avg(daily_avg_util) as monthly_avg_util, sum(daily_power) as monthly_power_kwh by host | eval monthly_power_kwh=round(monthly_power_kwh/1000,2)
cron_schedule = 0 6 1 * *
enableSched = 1
dispatch.earliest_time = -1mon
dispatch.latest_time = now
action.email = 1
action.email.to = dgx-reports@example.com
action.email.subject = DGX Spark Monthly Utilization Report
action.email.format = html
action.email.inline = 1

#=============================================================================
# LOOKUP GENERATING SEARCHES
#=============================================================================

[DGX Spark - Generate GPU Baseline Lookup]
description = Generate baseline metrics for anomaly detection
search = index=dgx_spark_metrics sourcetype=dcgm:metrics | stats avg(gpu_utilization) as baseline_util, avg(temperature) as baseline_temp, avg(power_usage) as baseline_power, stdev(gpu_utilization) as stdev_util, stdev(temperature) as stdev_temp by host, gpu | outputlookup dgx_spark_gpu_baseline.csv
cron_schedule = 0 0 * * *
enableSched = 1
dispatch.earliest_time = -7d
dispatch.latest_time = now
